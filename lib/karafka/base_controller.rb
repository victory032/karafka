# Karafka module namespace
module Karafka
  # Base controller from which all Karafka controllers should inherit
  # Similar to Rails controllers we can define before_enqueue callbacks
  # that will be executed
  # Note that if before_enqueue return false, the chain will be stopped and
  # the perform method won't be executed in sidekiq (won't peform_async it)
  # @example Create simple controller
  #   class ExamplesController < Karafka::BaseController
  #     def perform
  #       # some logic here
  #     end
  #   end
  #
  # @example Create a controller with a block before_enqueue
  #   class ExampleController < Karafka::BaseController
  #
  #     before_enqueue do
  #       # Here we should have some checking logic
  #       # If false is returned, won't schedule a perform action
  #     end
  #
  #     def perform
  #       # some logic here
  #     end
  #   end
  #
  # @example Create a controller with a method before_enqueue and topic
  #   class ExampleController < Karafka::BaseController
  #     self.topic = :kafka_topic
  #
  #     before_enqueue :before_method
  #
  #     def perform
  #       # some logic here
  #     end
  #
  #     private
  #
  #     def before_method
  #       # Here we should have some checking logic
  #       # If false is returned, won't schedule a perform action
  #     end
  #   end
  #
  # @example Create a controller with an after_failure action, topic and group
  #   class ExampleController < Karafka::BaseController
  #     self.group = :kafka_group_name
  #     self.topic = :kafka_topic
  #
  #     def perform
  #       # some logic here
  #     end
  #
  #     def after_failure
  #       # action taken in case perform fails
  #     end
  #   end
  #
  # @example Create a controller with a custom Sidekiq worker assigned. Note that if you use
  #   custom workers that does not inherit from Karafka::Workers::BaseWorker, perform method won't
  #   be exucted and you need to provide your business logic in the workers execution method.
  #
  #   class ExampleController < Karafka::BaseController
  #     self.worker = MyAppMainWorker
  #   end
  # @example Create a controller with a custom interchanger
  #
  #   class ExampleController < Karafka::BaseController
  #     self.interchanger = MyBinaryInterchanger
  #
  #     def perform
  #       # some logic here
  #     end
  #   end
  class BaseController
    include ActiveSupport::Callbacks

    # The schedule method is wrapped with a set of callbacks
    # We won't run perform at the backend if any of the callbacks
    # returns false
    # @see http://api.rubyonrails.org/classes/ActiveSupport/Callbacks/ClassMethods.html#method-i-get_callbacks
    define_callbacks :schedule,
      terminator: ->(_target, result) { result == false }

    class << self
      # group - Kafka cluster group name
      # topic - Kafka topic name
      # worker - custom worker class
      # parser - custom parser class
      # interchanger - custom params interchanger between controller and its worker
      attr_writer :group, :topic, :worker, :parser, :interchanger

      # @return [String, Symbol] group name for Kafka
      # @note If not set via self.group = 'group_name' it will be autogenerated based on
      #   the name and current controller topic
      def group
        @group ||= "#{Karafka::App.config.name.underscore}_#{topic}"
      end

      # @return [String, Symbol] topic on which we should listen for incoming messages
      # @note If not set via self.topic = 'topic_name' it will be autogenerated based on
      #   the controller name (including namespaces)
      def topic
        @topic ||= to_s.underscore.sub('_controller', '').tr('/', '_')
      end

      # @return [Parser] controller message parser
      # If not define, return JSON parser.
      # Parser should contain parse method and raise error that
      #   descends from Karafka::Errors::ParserError
      # @example:
      # class XmlParser
      #   class ParserError < Karafka::Errors::ParserError; end
      #
      #   def self.parse(message)
      #     Hash.from_xml(message)
      #   rescue REXML::ParseException
      #     raise ParserError
      #   end
      # end
      def parser
        @parser ||= JSON
      end

      # @return [Class] worker class to which we should schedule Sidekiq bg stuff
      # @note We use builder and conditionally assing, because we want to leave to users
      #   a possibility to use their own workers that don't inherit
      #   from Karafka::Workers::BaseWorker
      def worker
        @worker ||= Karafka::Workers::Builder.new(self).build
      end

      # @return [Class] interchanger between Karafka and Sidekiq - it allows to use interchangers
      #   that are slower than default JSON, however that don't break binary data
      #   #(or any other "weird" Kafka incoming messages content)
      # @note Default interchanges does literally nothing. It is made as a stub for custom
      #   interchangers logic
      def interchanger
        @interchanger ||= Karafka::Params::Interchanger
      end

      # Creates a callback that will be executed before scheduling to Sidekiq
      # @param method_name [Symbol, String] method name or nil if we plan to provide a block
      # @yield A block with a code that should be executed before scheduling
      # @note If value returned is false, will chalt the chain and not schedlue to Sidekiq
      # @example Define a block before_enqueue callback
      #   before_enqueue do
      #     # logic here
      #   end
      #
      # @example Define a class name before_enqueue callback
      #   before_enqueue :method_name
      def before_enqueue(method_name = nil, &block)
        set_callback :schedule, :before, method_name ? method_name : block
      end
    end

    # @raise [Karafka::Errors::TopicNotDefined] raised if we didn't define kafka topic
    # @raise [Karafka::Errors::PerformMethodNotDefined] raised if we didn't define the
    #   perform method and we use default built worker that requires it. If one will use
    #   custom worker, it won't be executed since the #perform logic then will be implemented
    #   and executed outside of default flow
    def initialize
      fail Errors::TopicNotDefined unless
        self.class.topic

      fail Errors::PerformMethodNotDefined if
        (self.class.worker <= Karafka::Workers::BaseWorker) && !respond_to?(:perform)
    end

    # Creates lazy loaded params object
    # @note Until first params usage, it won't parse data at all
    # @param message [Karafka::Connection::Message, Hash] message with raw content or a hash
    #   from Sidekiq that allows us to build params.
    def params=(message)
      @params = Karafka::Params::Params.build(message, self)
    end

    # Executes the default controller flow, runs callbacks and if not halted
    # will schedule a perform task in sidekiq
    def schedule
      run_callbacks :schedule do
        perform_async
      end
    end

    private

    # @return [Karafka::Params::Params] Karafka params that is a hash with indifferent access
    # @note Params internally are lazy loaded before first use. That way we can skip parsing
    #   process if we have before_enqueue that rejects some incoming messages without using params
    #   It can be also used when handling really heavy data (in terms of parsing). Without direct
    #   usage outside of worker scope, it will pass raw data into sidekiq, so we won't use Karafka
    #   working time to parse this data. It will happen only in the worker (where it can take time)
    #   that way Karafka will be able to process data really quickly. On the other hand, if we
    #   decide to use params somewhere before it hits worker logic, it won't parse it again in
    #   the worker - it will use already loaded data and pass it to Redis
    # @note Invokation of this method will cause load all the data into params object. If you want
    #   to get access without parsing, please access @params directly
    def params
      @params.fetch
    end

    # Enqueues the execution of perform method into a worker.
    # @note Each worker needs to have a class #perform_async method that will allow us to pass
    #   parameters into it. We always pass controller class as a first argument and this request
    #   params as a second one
    def perform_async
      Karafka.monitor.notice(
        self.class,
        params: @params,
        worker: self.class.worker,
        interchanger: self.class.interchanger
      )

      # We use @params directly (instead of #params) because of lazy loading logic that is behind
      # it. See Karafka::Params::Params class for more details about that
      self.class.worker.perform_async(
        self.class,
        self.class.interchanger.load(@params)
      )
    end
  end
end
